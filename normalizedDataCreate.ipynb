{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create demean and unit variance data and save in h5 (w/ sliding window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qeek/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int16 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from sklearn import preprocessing \n",
    "import h5py\n",
    "from scipy.misc import imresize\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def sliding_window(image, step, windowSize):\n",
    "    image = np.asarray(image)\n",
    "    for y in range(0, image.shape[0], step):\n",
    "        for x in range(0, image.shape[1], step):\n",
    "            yield (x, y, image[y:y+windowSize[1], x:x+windowSize[0]])\n",
    "            \n",
    "            \n",
    "directory = '/run/user/1000/gvfs/smb-share:server=192.168.200.1,share=mri'\n",
    "step = 20\n",
    "windowSize = (64, 64)\n",
    "\n",
    "mri_df = pd.read_csv(os.path.join(directory, 'MRI.csv'))\n",
    "imgList = mri_df.image.tolist()\n",
    "gtList = mri_df.target.tolist()\n",
    "filenames = []\n",
    "images = np.empty([1, windowSize[0], windowSize[1]])\n",
    "targets = np.empty([1, windowSize[0], windowSize[1]], dtype=int)\n",
    "\n",
    "# This is for 20180416 data(chest data are in delList)\n",
    "delList = [1 ,5, 8, 14, 20, 21]\n",
    "for i, f in enumerate(imgList):\n",
    "    if i not in delList:\n",
    "        # Get gt and image file \n",
    "        img_data = nib.load(os.path.join(directory, 'MRI', f)).get_data()\n",
    "        gt_data = nib.load(os.path.join(directory, 'MRI', gtList[i])).get_data()\n",
    "\n",
    "        # slice every row from top to bottom\n",
    "        for d in range(gt_data.shape[2]):\n",
    "            target = gt_data[:, :, d]\n",
    "            image = img_data[:, :, d]\n",
    "            scaler = preprocessing.StandardScaler().fit(image)\n",
    "            image_norm = scaler.transform(image)\n",
    "            if target.sum() > 0:\n",
    "                windowCount = 0\n",
    "                for (x, y, gt_patch) in sliding_window(target, step=step, windowSize=windowSize):\n",
    "                    if gt_patch.sum() > 200:\n",
    "                        window_filename = '{:0>4d}_{:0>3d}_{:0>5d}'.format(i, d, windowCount)\n",
    "                        filenames.append(window_filename)\n",
    "                        img_patch = image_norm[y:y+windowSize[1], x:x+windowSize[0]]\n",
    "                        img_patch = np.expand_dims(img_patch, axis=0)\n",
    "                        gt_patch = np.expand_dims(gt_patch, axis=0)\n",
    "                        images = np.concatenate((images, img_patch), axis=0)\n",
    "                        targets = np.concatenate((targets, gt_patch), axis=0)\n",
    "                        windowCount += 1\n",
    "\n",
    "images = images[1:, :, :]\n",
    "targets = targets[1:, :, :]\n",
    "filenames = [f.encode('utf8') for f in filenames]\n",
    "\n",
    "hf = h5py.File('data.h5', 'w')\n",
    "hf.create_dataset('filenames', data=filenames)\n",
    "hf.create_dataset('images', data=images)\n",
    "hf.create_dataset('targets', data=targets)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create demean and unit variance data and save in h5 (w/o sliding window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case2015.03.30.13.36.40_CHIU_20150330_11527_002_1_21_CYBER_KNIFE.nii\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qeek/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "/home/qeek/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "/home/qeek/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case2016.11.04.13.54.03_CHANG_20161104_1631_005_1_21_CYBER_KNIFE_C+.nii\n",
      "case2011.04.18.13.08.02_YANG_20110418_4663_002_1_21_CYBER_KNIFE.nii\n",
      "case2015.06.16.13.40.29_TSAI_20150616_16261_004_1_21_CYBER_KNIFE.nii\n",
      "case2010.07.26.13.18.50_SOUNG_20100726_46207_005_1_21_CYBER_KNIFE_C+.nii\n",
      "case2011.01.18.13.31.24_CHEN_20110118_1902_004_1_21_CYBER_KNIFE_C+.nii\n",
      "case2010.02.08.11.30.30_JANG_20100208_28496_003_CYBERKNIFE__2_AX_T1-2MM+C.nii\n",
      "case2011.01.18.13.03.31_CHEN_20110118_34619_004_0517349__AX_T1_+C.nii\n",
      "case2015.03.30.13.37.46_CHIU_20150330_11527_004_1_21_CYBER_KNIFE.nii\n",
      "case2010.02.08.11.10.59_JANG_20100208_28496_002_CYBERKNIFE_4_3D_FIESTA-AX.nii\n",
      "case2011.01.18.13.27.12_CHEN_20110118_1902_002_1_21_CYBER_KNIFE.nii\n",
      "case2011.04.18.12.31.23_YANG_20110418_36224_002_CYBERKNIFE__3D_FIESTA-C.nii\n",
      "case2016.11.04.13.52.45_CHANG_20161104_1631_002_1_21_CYBER_KNIFE.nii\n",
      "case2009.03.04.13.18.10_LO_20090304_29218_002_1_21_CYBER_KNIFE_C+.nii\n",
      "case2011.01.18.12.58.13_CHEN_20110118_34619_003_0517349__AX_T1.nii\n",
      "case2011.04.18.13.30.56_YANG_20110418_4663_006_1_24_CTA_+_Perfusion.nii\n",
      "case2010.02.08.13.14.33_JANG_20100208_40740_002_1_21_CYBER_KNIFE_C+.nii\n",
      "case2015.06.16.13.39.24_TSAI_20150616_16261_002_1_21_CYBER_KNIFE.nii\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from sklearn import preprocessing \n",
    "import h5py\n",
    "from scipy.misc import imresize\n",
    "import pandas as pd\n",
    "          \n",
    "directory = '/run/user/1000/gvfs/smb-share:server=192.168.200.1,share=mri'\n",
    "\n",
    "mri_df = pd.read_csv(os.path.join(directory, 'MRI.csv'))\n",
    "imgList = mri_df.image.tolist()\n",
    "gtList = mri_df.target.tolist()\n",
    "filenames = []\n",
    "imSize = (256, 256)\n",
    "\n",
    "# Arrays used for stacking images\n",
    "images = np.empty([1, imSize[0], imSize[1]])\n",
    "image_norms = np.empty([1, imSize[0], imSize[1]])\n",
    "targets = np.empty([1, imSize[0], imSize[1]], dtype=int)\n",
    "\n",
    "# This is for 20180416 data(chest data are in delList)\n",
    "delList = [1 ,5, 8, 14, 20, 21]\n",
    "for i, f in enumerate(imgList):\n",
    "    if i not in delList:\n",
    "        print (f)\n",
    "        # Get gt and image file \n",
    "        img_data = nib.load(os.path.join(directory, 'MRI', f)).get_data()\n",
    "        gt_data = nib.load(os.path.join(directory, 'MRI', gtList[i])).get_data()\n",
    "        \n",
    "        # slice every row from top to bottom\n",
    "        for d in range(gt_data.shape[2]):\n",
    "            # Read slice and resize every slice to imSize\n",
    "            target = gt_data[:, :, d]\n",
    "            image = img_data[:, :, d]\n",
    "            image = imresize(image, imSize)\n",
    "            target = imresize(target, imSize, 'nearest', mode='F')\n",
    "            \n",
    "            # Demean and unit variance\n",
    "            scaler = preprocessing.StandardScaler().fit(image)\n",
    "            image_norm = scaler.transform(image)\n",
    "\n",
    "            # Write to numpy array\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            image_norm = np.expand_dims(image_norm, axis=0)\n",
    "            target = np.expand_dims(target, axis=0)\n",
    "            \n",
    "            images = np.concatenate((images, image), axis=0)\n",
    "            image_norms = np.concatenate((image_norms, image_norm), axis=0)\n",
    "            targets = np.concatenate((targets, target), axis=0)\n",
    "            \n",
    "            # Record slice filename\n",
    "            im_filename = '{:0>4d}_{:0>3d}'.format(i, d)\n",
    "            filenames.append(im_filename)\n",
    "\n",
    "images = images[1:, :, :]\n",
    "image_norms = image_norms[1:, :, :]\n",
    "targets = targets[1:, :, :]\n",
    "filenames = [f.encode('utf8') for f in filenames]\n",
    "\n",
    "hf = h5py.File('data/data.h5', 'w')\n",
    "hf.create_dataset('filenames', data=filenames)\n",
    "hf.create_dataset('images', data=images)\n",
    "hf.create_dataset('image_norms', data=image_norms)\n",
    "hf.create_dataset('targets', data=targets)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1/5 data for validation, and 4/5 for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "\n",
    "hf = h5py.File('data/data.h5', 'r')\n",
    "filenames = hf.get('filenames')\n",
    "images = hf.get('images')\n",
    "image_norms = hf.get('image_norms')\n",
    "targets = hf.get('targets')\n",
    "\n",
    "valNum = int(len(filenames)/5)\n",
    "\n",
    "hf_train = h5py.File('data/data-train.h5', 'w')\n",
    "hf_train.create_dataset('filenames',   data=filenames[valNum:])\n",
    "hf_train.create_dataset('images',      data=images[valNum:, :, :])\n",
    "hf_train.create_dataset('image_norms', data=image_norms[valNum:, :, :])\n",
    "hf_train.create_dataset('targets',     data=targets[valNum:, :, :])\n",
    "hf_train.close()\n",
    "\n",
    "hf_val = h5py.File('data/data-val.h5', 'w')\n",
    "hf_val.create_dataset('filenames',   data=filenames[:valNum])\n",
    "hf_val.create_dataset('images',      data=images[:valNum, :, :])\n",
    "hf_val.create_dataset('image_norms', data=image_norms[:valNum, :, :])\n",
    "hf_val.create_dataset('targets',     data=targets[:valNum, :, :])\n",
    "hf_val.close()\n",
    "\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train data: (2492, 256, 256)\n",
      "Size of val data: (622, 256, 256)\n",
      "Total num: 3114\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "num = 0\n",
    "split = ['train', 'val']\n",
    "for s in split:\n",
    "    hf = h5py.File(os.path.join(os.getcwd(), 'data', 'data-' + s + '.h5'), 'r')\n",
    "    targets = hf['targets'][:]\n",
    "    num += targets.shape[0]\n",
    "    print ('Size of {} data: {}'.format(s, targets.shape))\n",
    "\n",
    "print ('Total num: {}'.format(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===========================            \n",
    "# directory = '/run/user/1000/gvfs/smb-share:server=192.168.200.1,share=mri'\n",
    "# file_name = 'case2009.03.04.13.18.10_LO_20090304_29218_002_1_21_CYBER_KNIFE_C+.nii'\n",
    "# nii_file = os.path.join(directory, 'MRI', file_name)\n",
    "# img = nib.load(nii_file)\n",
    "# # print (img.header)\n",
    "# img_data = img.get_data()[:, :, 128]\n",
    "# scaler = preprocessing.StandardScaler().fit(img_data)\n",
    "# # print (scaler.mean_)\n",
    "# # print (scaler.scale_)\n",
    "# img_data_norm = scaler.transform(img_data)\n",
    "# img_data = Image.fromarray(img_data).convert('L')\n",
    "# img_data.save(os.path.join(directory, 'test.png'))\n",
    "\n",
    "# # print (img_data_norm.mean())\n",
    "# # print (img_data_norm.std())\n",
    "# print (img_data_norm)\n",
    "# img_data_norm = Image.fromarray(img_data_norm).convert('L')\n",
    "# img_data_norm.save(os.path.join(directory, 'test_norm.png'))\n",
    "# ppimage = Image.open(os.path.join(directory, 'test_norm.png'))\n",
    "# print (ppimage.getdata())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
